{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_qWgbSrNdjn"
      },
      "source": [
        "# Introduzione a Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M0crmii8Ysb"
      },
      "source": [
        "Keras è una libreria di alto livello che consente di costruire e sperimentare con modelli di Deep Learning in maniera flessibile.\n",
        "E' integrata con TensorFlow, che fornisce un supporto più di \"basso livello\". Documentazione, tutorial ed esempi sono nel sito web di TF.\n",
        "\n",
        "Documentazione API (andate su `tf.keras`): https://www.tensorflow.org/versions\n",
        "\n",
        "Guide Keras (per componenti API specifici): https://www.tensorflow.org/guide/keras\n",
        "\n",
        "Tutorials (coprono esempi di utilizzo base): https://www.tensorflow.org/tutorials/keras\n",
        "\n",
        "**Warning:** abbiamo già visto cosa comporta utilizzare TensorFlow in situazioni normali. In generale, è meglio utilizzare Keras ove possibile. Utilizzate TensorFlow solo quando non avete alternative (dovete creare qualcosa di più complesso di ciò che Keras vi mette a disposizione)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8Q3zv6RjiUt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras as K\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y6WrRhvgA9x3",
        "outputId": "3c5ca909-663a-47c7-d868-2fc1b3c00852"
      },
      "outputs": [],
      "source": [
        "K.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UVVH9-QcC9wT",
        "outputId": "744d73a1-b537-4872-f27d-08ce47898ea8"
      },
      "outputs": [],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjdGEixplbmc"
      },
      "source": [
        "# Primo training end-to-end con Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI7FoTtS_zPa"
      },
      "source": [
        "Come l'altra volta, cominciamo dal caricare il nostro dataset di immagini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY6DQMRuldzC"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.datasets as kds\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bna7Yrs6lgai",
        "outputId": "4e937c41-eeb3-4d9d-d1e9-bab0ee6b2e9d"
      },
      "outputs": [],
      "source": [
        "(train_X, train_y), (test_X, test_y) = kds.mnist.load_data(path='ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYgUq7q3_zPd",
        "outputId": "63abce63-5671-45f5-adb4-d93c3a578f21"
      },
      "outputs": [],
      "source": [
        "type(train_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCq1BhcSliBq",
        "outputId": "43ad73cf-1998-45a3-a2ad-44e5f1bad441"
      },
      "outputs": [],
      "source": [
        "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "MjCYYuZE_zPe",
        "outputId": "b3afa6d4-990b-436d-928f-ed8e748ee4f1"
      },
      "outputs": [],
      "source": [
        "print(f\"Label is {train_y[0]}\")\n",
        "Image.fromarray(train_X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bihyoidhljTS",
        "outputId": "44a37b0e-db04-45cc-9680-930e1748d518"
      },
      "outputs": [],
      "source": [
        "train_X.dtype, train_y.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "724rh3cs_zPf"
      },
      "source": [
        "E non ci scordiamo mai di fare il preprocessing..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26UpaZ7A9E3V"
      },
      "source": [
        "## Keras Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPik3nwM9yVh"
      },
      "source": [
        "La prima cosa da fare con Keras è costruire la nostra \"__scatola__\" che ci permetterà di processare i dati da input ad output.\n",
        "\n",
        "Vediamo il primo modo di costruirla, che è tramite la __Sequential API__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alvksVvWk4vs"
      },
      "outputs": [],
      "source": [
        "model = K.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9m41arx_zPh"
      },
      "source": [
        "Una pipeline è composta da **layers**.\n",
        "\n",
        "__Un layer è una funzione che, dato un input, restituisce un output che è il risultato della \"trasformazione\" dell'input__. Solitamente (ma, come vedremo fra un attimo, non necessariamente), questo avviene rispetto a parametri adattivi.\n",
        "\n",
        "Un modello si può costruire componendo molti layer, e mette a disposizione anche interfacce per funzionalità più complesse come il training, l'inferenza, ecc...\n",
        "\n",
        "Dentro la Sequential API, possiamo inserire quanti layer vogliamo semplicemente utilizzando `model.add(new_layer)`. __Tutti i layer verranno eseguiti nell'ordine in cui li abbiamo inseriti__.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fu2yYY8_zPh"
      },
      "source": [
        "### Flatten Layer\n",
        "\n",
        "Questo layer ci permette di appiattire i tensori, portandoli da un formato 28x28 a un vettore di lunghezza 784."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcWuYNtw_zPi"
      },
      "outputs": [],
      "source": [
        "flatten = K.layers.Flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTXkGB4x_zPi"
      },
      "source": [
        "### Rescaling Layer\n",
        "\n",
        "Questo layer ci permette di fare lo stesso lavoro che abbiamo fatto la volta scorsa quando abbiamo diviso i nostri dati per 255. Questa volta, però, sarà il layer stesso a occuparsene in autonomia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY4XGyK1_zPi",
        "outputId": "df741a80-ab3d-4358-d685-cf5eb9624272"
      },
      "outputs": [],
      "source": [
        "help(K.layers.Rescaling.__init__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHhGXV4t_zPj"
      },
      "outputs": [],
      "source": [
        "rescaling = K.layers.Rescaling(scale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9uhiM8-_zPj"
      },
      "source": [
        "### Dense Layer\n",
        "\n",
        "Questo layer ci permette di implementare qualsiasi funzione che prevede:\n",
        "\n",
        "1. Una funzione lineare del tipo `y = Wx + b`, come quella che abbiamo visto la volta scorsa;\n",
        "2. Successivamente, una __funzione di attivazione__, che vedremo successivamente e ci permetterà di creare le nostre reti neurali."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fft2dXhg_zPj",
        "outputId": "2b29f38d-494b-48a8-8c3b-084752afe7dc"
      },
      "outputs": [],
      "source": [
        "help(K.layers.Dense.__init__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fOmYJUc_zPk"
      },
      "outputs": [],
      "source": [
        "linear = K.layers.Dense(10, activation='softmax') # E la dimensione di input?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tra i parametri del Dense layer ne troviamo alcuni con la voce \"regularizer\". La __regolarizzazione__ è uno strumento fondamentale per ridurre la probabilità di __overfitting__. \n",
        "\n",
        "Generalmente, valori molto grandi dei pesi di un modello sono associati all'overfitting. Di conseguenza, aggiungendo un termine di penalizzazione alla loss è possibile indurre i pesi del modello ad avere valori più piccoli.\n",
        "A seconda del tipo di penalizzazione, possiamo avere L1 (valore assoluto) o L2 (norma quadratica)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "l2_reg = K.regularizers.l2(l2=0.5)\n",
        "regularized_linear = K.layers.Dense(10, activation='softmax', kernel_regularizer=l2_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJD156-u_zPk"
      },
      "source": [
        "### Componiamo la pipeline\n",
        "\n",
        "Come abbiamo già detto, è sufficiente chiamare la funzione `model.add` una volta per ogni layer che vogliamo inserire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyy_kLDu_zPk"
      },
      "outputs": [],
      "source": [
        "model.add(flatten)\n",
        "model.add(rescaling)\n",
        "model.add(linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functional API: un'alternativa alla Sequential API\n",
        "\n",
        "Nella functional API non abbiamo bisogno di costruire a priori quella \"scatola\" della Sequential API. La cosa che è importante fissare a priori è un `InputLayer`, che si comporta da segnaposto rispetto a degli input futuri che riceveremo.\n",
        "\n",
        "Dopodiché, ad ogni layer che creiamo possiamo dare in input un \"output immaginario\" del layer precedente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = K.Input(shape=(28, 28)) # Implicitamente, la dimensione sarà (None, 28, 28)\n",
        "flattened = K.layers.Flatten()(inputs)\n",
        "rescaled = K.layers.Rescaling(scale=1/255.)(flattened)\n",
        "outputs = K.layers.Dense(units=10, activation='softmax')(rescaled)\n",
        "model = K.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JntXHeE_zPk"
      },
      "source": [
        "### Compiliamo la pipeline con un ottimizzatore\n",
        "\n",
        "La funzione di compilazione è quella che ci permette di prendere tutti questi pezzetti che stiamo mettendo dentro la Sequential API, e \"incastonarli\" dentro un grafo computazionale.\n",
        "\n",
        "Sarà Keras stesso, a questo punto, a curarsi di quali sono le parti che hanno dei parametri adattivi (che quindi hanno bisogno di gradiente) e quali no, rispetto al modo in cui sono definiti i layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMwPvPZG_zPk",
        "outputId": "6449e553-2738-467f-9c64-0e30a536fecc"
      },
      "outputs": [],
      "source": [
        "help(model.compile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhvHTj_m_zPl"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=K.optimizers.Adam(learning_rate=0.1),\n",
        "    loss=K.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMWbtPQ__zPl"
      },
      "source": [
        "### Alleniamo il modello"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JU4l9sFW_zPl"
      },
      "source": [
        "Per allenare un modello dentro Keras, invece di dover riscrivere tutto il loop di training come abbiamo fatto la scorsa volta, è sufficiente chiamare il metodo `.fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5OQBSSN_zPl",
        "outputId": "f9ddf0c0-32b0-4b9e-f9a0-a5b91e07d603"
      },
      "outputs": [],
      "source": [
        "help(model.fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzFV8o1w_zPl",
        "outputId": "a806b6f8-41b1-4d9c-f708-59e50f6bf4d7"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x=train_X,\n",
        "    y=train_y,\n",
        "    epochs=500,\n",
        "    batch_size=train_X.shape[0]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__E se lo regolarizzassimo?__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy-tAQe5j6ub",
        "outputId": "afe0de7b-2c6d-40b3-c4d5-6b38754aa4fa"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T5A0Vom_zPm"
      },
      "source": [
        "Arrivati a questo punto, abbiamo un'implementazione che è esattamente equivalente a quella di TensorFlow che abbiamo utilizzato l'ultima volta, ma possiamo facilmente vedere che la quantità di righe di codice è nettamente inferiore, e di gran lunga meno \"tecnica\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdXmZG_g_zPm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tf_train_X = tf.cast(train_X, dtype=tf.float32) / 255\n",
        "tf_train_X = tf.reshape(tf_train_X, [train_X.shape[0], -1])\n",
        "W, b = tf.Variable(tf.random.normal((784, 10))), tf.Variable(tf.zeros(10))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "epochs = 500\n",
        "\n",
        "for e in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        prediction = tf.nn.softmax(tf_train_X @ W + b, axis=-1)\n",
        "        loss_value = loss_fn(train_y, prediction)\n",
        "    \n",
        "    grads = tape.gradient(loss_value, [W, b])\n",
        "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        prediction = tf.nn.softmax(tf_train_X @ W + b, axis=-1)\n",
        "        print(f\"Epoca {e}: accuratezza = {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBgXLeSw_zPn"
      },
      "outputs": [],
      "source": [
        "model = K.Sequential()\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.Rescaling(scale=1/255.))\n",
        "model.add(K.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=K.optimizers.Adam(learning_rate=0.1),\n",
        "    loss=K.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(x=train_X, y=train_y, epochs=500, batch_size=train_X.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nonostante sembrino equivalenti, abbiamo già qualcosa in più da ciò che ci viene restituito dalla funzione `.fit`. Questa funzione restituisce uno storico di ciò che è accaduto durante il training in termini di metriche."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Questo è molto utile per poter visualizzare il comportamento del modello dopo il training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax = sns.lineplot(x=history.epoch, y=history.history['loss'])\n",
        "ax.set(xlabel='epoch', ylabel='loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ax = sns.lineplot(x=history.epoch, y=history.history['accuracy'])\n",
        "ax.set(xlabel='epoch', ylabel='accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0_ayYvB_zPn"
      },
      "source": [
        "Finora abbiamo sempre lavorato sul training set, ma come va il nostro modello su dati su cui non si è mai allenato? Vediamolo tramite la funzione `.evaluate`, che ci restituisce i risultati delle metriche sui dati che forniamo al metodo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj4Rc2Tj_zPn",
        "outputId": "56b5ba0e-eb8d-4016-9821-d745dd9dc252"
      },
      "outputs": [],
      "source": [
        "metrics = model.evaluate(test_X, test_y)\n",
        "metrics # loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se vogliamo, possiamo anche farci restituire le predizioni su dati nuovi tramite la funzione `.predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model.predict(test_X)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In maniera equivalente, possiamo chiamare l'oggetto del modello come se fosse una funzione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = model(test_X)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfVNQGxN_zPo"
      },
      "source": [
        "Comunque, con un dataset giocattolo come MNIST possiamo sicuramente fare di meglio, ed è qui che entrano in gioco le __reti neurali deep__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJaQs9pB_zPo"
      },
      "source": [
        "### Classificazione su MNIST con una rete neurale deep\n",
        "\n",
        "Per far diventare il nostro modello precedente una rete neurale deep, è sufficiente aggiungere layer intermedi __non lineari__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qNozJyt_zPo",
        "outputId": "ebcb7be6-ccab-4cf9-c094-d437293ae1d1"
      },
      "outputs": [],
      "source": [
        "model = K.Sequential()\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.Rescaling(scale=1/255.))\n",
        "model.add(K.layers.Dense(1000, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(200, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=K.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=K.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(x=train_X, y=train_y, epochs=500, batch_size=train_X.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdGaD_sUAVzw",
        "outputId": "b3f195db-36cf-4826-d14b-f3910156c7a3"
      },
      "outputs": [],
      "source": [
        "metrics = model.evaluate(test_X, test_y)\n",
        "metrics # loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your turn!\n",
        "\n",
        "Riprendiamo il problema dell'ultima volta, provate a implementare una rete neurale deep con Keras, e allenatela in GPU.\n",
        "Il modello deve avere:\n",
        "\n",
        "1. Un layer da 300 unità con attivazione `tanh`;\n",
        "2. Un layer da 100 unità con attivazione `tanh`;\n",
        "3. Un layer da 1 unità senza attivazione.\n",
        "\n",
        "Questa volta, invece di utilizzare il metodo di standardizzazione di sklearn (per intenderci, lo `StandardScaler`), utilizzate il layer [`Normalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) di Keras. E' sufficiente, invece di chiamare la funzione `.fit` dello StandardScaler, che chiamiate la funzione `.adapt` del layer che create sui dati di training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(h__train_X, h_train_Y), (h_test_X, h_test_y) = tf.keras.datasets.boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qdJgSv_j_1d"
      },
      "source": [
        "# Validazione ed Early Stopping\n",
        "\n",
        "La cosa che abbiamo appena fatto, in realtà, non va __MAI__ fatta. I dati di test sono quelli che, una volta che abbiamo scelto il modello migliore*, ci permettono di valutare la qualità del modello su un campione di dati reali. Se li utilizziamo prima e vediamo le metriche, stiamo automaticamente \"barando\".\n",
        "\n",
        "*La scelta del modello migliore avviene rispetto a diversi iperparametri (ovvero diverse configurazioni del modello, e lo vedremo la prossima volta) e rispetto al singolo processo di training (ovvero, ci fermiamo in un momento del training in cui il modello appare \"particolarmente buono\").\n",
        "\n",
        "Di conseguenza, durante il training delle reti neurali, è fondamentale avere uno strumento che ci permetta di monitorare in che modo si potrebbe comportare il nostro modello se dovesse predire dei dati su cui non si sta allenando durante una generica epoca di training. Solitamente, questo strumento viene rappresentato dal __validation set__, su cui facciamo delle predizioni, valutiamo le metriche di quelle predizioni, __ma non ci alleniamo mai__. Questo rappresenta il metodo più comune per valutare la qualità del modello durante la fase di learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La prima cosa da fare, è prendere il nostro training set e tirar fuori un pezzetto (solitamente tra il 5% e il 20%) da utilizzare come validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X, eval_X, train_y, eval_y = train_test_split(\n",
        "    train_X.numpy(), train_y,\n",
        "    test_size=0.15,\n",
        "    shuffle=True,\n",
        "    stratify=train_y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X.shape, train_y.shape, eval_X.shape, eval_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = K.Sequential()\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.Rescaling(scale=1/255.))\n",
        "#model.add(K.layers.Dense(1000, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(200, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=K.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=K.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x=train_X, \n",
        "    y=train_y, \n",
        "    epochs=100, \n",
        "    batch_size=1000, # Questa volta cambiamo il batch size\n",
        "    validation_data=(eval_X, eval_y)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "list(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = pd.DataFrame({\n",
        "    'epoch': history.epoch,\n",
        "    'loss': history.history['loss'],\n",
        "    'val_loss': history.history['val_loss']\n",
        "})\n",
        "ax = sns.lineplot(x='epoch', y='value', hue='variable', data=pd.melt(results, ['epoch']))\n",
        "ax.set(xlabel='epoch', ylabel='loss value')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = pd.DataFrame({\n",
        "    'epoch': history.epoch,\n",
        "    'loss': history.history['accuracy'],\n",
        "    'val_loss': history.history['val_accuracy']\n",
        "})\n",
        "ax = sns.lineplot(x='epoch', y='value', hue='variable', data=pd.melt(results, ['epoch']))\n",
        "ax.set(xlabel='epoch', ylabel='accuracy value')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ora che abbiamo il validation set, possiamo anche utilizzarlo per decidere quando fermare il training, __senza preoccuparci del numero massimo di epoche__.\n",
        "\n",
        "Keras implementa l'early stopping sotto forma di `callback`. Le callbacks sono funzioni che vengono chiamate in momenti particolari del training, per esempio prima dell'inizio di un'epoca, alla fine di un'epoca, dopo un generico step di learning etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping = K.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=5,\n",
        "    min_delta=0.0005,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = K.Sequential()\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.Rescaling(scale=1/255.))\n",
        "#model.add(K.layers.Dense(1000, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(200, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=K.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=K.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x=train_X, \n",
        "    y=train_y, \n",
        "    epochs=10000, \n",
        "    batch_size=1000,\n",
        "    validation_data=(eval_X, eval_y),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxk-4jYSyaCP"
      },
      "source": [
        "## Salvataggio e caricamento del modello"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBMcK8_nIf5F"
      },
      "source": [
        "Ora che abbiamo un modello che potremmo anche voler riutilizzare, avere un metodo per il salvataggio e il caricamento è fondamentale.\n",
        "\n",
        "La  `model serialization` può essere utile anche quando usate colab, visto che il tempo di utilizzo è limitato e si può disconnettere nel mezzo del processo e potreste voler riprendere il training in seguito."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEdOYZpCycm2"
      },
      "outputs": [],
      "source": [
        "model.save('my_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_model = K.models.load_model('my_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlP-jZzBzQcg"
      },
      "source": [
        "Ci sono parecchie altre opzioni. Salvataggio in formato H5, salvare solo i pesi, salvare solo l'architettura del modello, etc...  \n",
        "Guida alla serializzazione dei modelli: https://www.tensorflow.org/guide/keras/save_and_serialize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gF499iBuw8l"
      },
      "source": [
        "# Monitoring degli esperimenti con TensorBoard\n",
        "\n",
        "Per comprendere ciò che avviene durante il training, è importante fare logging di valori, creare grafici e così via. \n",
        "\n",
        "**Tenere da parte un log di testo può sempre tornare utile, potreste voler creare visualizzazioni \"custom\".**\n",
        "\n",
        "Esistono alcuni strumenti che facilitano il procedimento, tra cui TensorBoard, che fornisce un'interfaccia web da cui controllare l'andamento del training del nostro modello."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYiCaNoPvLTD"
      },
      "outputs": [],
      "source": [
        "# this is only needed in a notebook\n",
        "%load_ext tensorboard "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49jBCaVAv9IU"
      },
      "outputs": [],
      "source": [
        "tensorboard_callback = K.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir ./logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = K.Sequential()\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.Rescaling(scale=1/255.))\n",
        "#model.add(K.layers.Dense(1000, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(200, activation='relu')) # Nuovo layer non lineare\n",
        "model.add(K.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=K.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=K.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x=train_X, \n",
        "    y=train_y, \n",
        "    epochs=10000, \n",
        "    batch_size=1000,\n",
        "    validation_data=(eval_X, eval_y),\n",
        "    callbacks=[early_stopping, tensorboard_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Your Turn!\n",
        "\n",
        "Dato lo split fornito di seguito, completate l'esercizio precedente aggiungendo anche il validation set e l'early stopping alla funzione `.fit`.\n",
        "\n",
        "Opzionale: visualizzate il tutto anche con la TensorBoard!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(h_train_X, h_train_y), (h_test_X, h_test_y) = tf.keras.datasets.boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "h_train = np.concatenate([h_train_X, h_train_y[:, np.newaxis]], axis=1)\n",
        "random.seed(42)\n",
        "random.shuffle(h_train)\n",
        "\n",
        "h_train, h_eval = h_train[75:], h_train[:75]\n",
        "h_train_X, h_train_y = h_train[:, :-1], h_train[:, -1]\n",
        "h_eval_X, h_eval_y = h_eval[:, :-1], h_eval[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(h_train_X.shape, h_train_y.shape), (h_eval_X.shape, h_eval_y.shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('tf_mela')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3917c018cfd92834e25e8ac39593c8ba50cba5d103d91e5ca7d5a867c45ae1af"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
