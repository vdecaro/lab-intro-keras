{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benvenuti al laboratorio di Deep Learning! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basi di manipolazione di tensori\n",
    "\n",
    "Un tensore possiamo vederlo come una matrice multidimensionale, dove il numero di dimensioni è definito a priori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creazione di tensori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tensore a zero dimensioni è uno __scalare__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = tf.constant(0)\n",
    "print(f\"Valore del tensore = {scalar}\")\n",
    "print(f\"Numero di dimensioni del tensore = {len(scalar.shape)}\")\n",
    "print(f\"Forma del tensore = {scalar.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tensore di una dimensione è un __vettore__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.constant([1, 2, 3])\n",
    "print(f\"Valore del tensore = {vector}\")\n",
    "print(f\"Numero di dimensioni del tensore = {len(vector.shape)}\")\n",
    "print(f\"Forma del tensore = {vector.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tensore di due dimensioni è una __matrice__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tf.constant(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 9]]\n",
    ")\n",
    "print(f\"Valore del tensore = \\n {matrix}\")\n",
    "print(f\"Numero di dimensioni del tensore = {len(matrix.shape)}\")\n",
    "print(f\"Forma del tensore = {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tensore da n dimensioni con n > 2 è... un __tensore__ n-dimensionale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "tensor = tf.random.normal(tuple(3 for i in range(n)))\n",
    "print(f\"Valore del tensore = \\n {tensor}\")\n",
    "print(f\"Numero di dimensioni del tensore = {len(tensor.shape)}\")\n",
    "print(f\"Forma del tensore = {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogni tensore è caratterizzato anche da un __tipo__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... e possiamo anche cambiarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_tensor = tf.cast(tensor, dtype=tf.int32)\n",
    "print(int_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor indexing e slicing\n",
    "\n",
    "Ok, abbiamo i nostri tensori. Molto spesso ci capita di voler prendere solo un elemento di un tensore, prenderne solo un pezzo e, magari, modificarne un altro.\n",
    "\n",
    "Indexing e slicing ci permettono di selezionare le parti che vogliamo del tensore, e farci ciò di cui necessitiamo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cominciamo dal prendere un solo elemento. Avendo un tensore di tre dimensioni:\n",
    "1. Se vogliamo prendere un solo scalare dobbiamo inserire un valore per __tutte__ le dimensioni;\n",
    "2. Se vogliamo ricavare un vettore, dobbiamo inserire un valore per __due delle tre__ dimensioni;\n",
    "3. Se vogliamo ricavare una matrice, dobbiamo inserire un valore per __una sola__ delle dimensioni.\n",
    "\n",
    "Maggiore è il numero delle dimensioni, più possibilità avremo nella scelta degli indici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Scalare = {tensor[0, 1, 0]}\")\n",
    "print(f\"Vettore = {tensor[:, 0, -1]}\") # che è 'sto -1?\n",
    "print(f\"Matrice = {tensor[:, 2]}\") # E l'ultima dimensione?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo slicing, che corrisponde ad \"affettare\" in italiano, ci consente di prendere, appunto, delle \"fette\" di un tensore. Lo slicing possiamo farlo con quella notazione `:` che abbiamo appena visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prendo solo una fetta rispetto alla prima dimensione:\")\n",
    "tensor_slice = tensor[1:]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Forma di tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Numero dimensioni = {len(tensor_slice.shape)}\") # Quante dimensioni avrà il tensore risultante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo slicing può essere fatto anche rispetto a più dimensioni, e coordinato anche con indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Affetto la prima e l'ultima dimensione:\")\n",
    "tensor_slice = tensor[1:, :, -1:]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Forma di tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Numero dimensioni = {len(tensor_slice.shape)}\") # Quante dimensioni avrà il tensore risultante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Affetto la prima e l'ultima dimensione:\")\n",
    "tensor_slice = tensor[1:, :, 2]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Forma di tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Numero dimensioni = {len(tensor_slice.shape)}\") # Questa volta quante dimensioni avrà il tensore risultante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenazione e stacking di tensori\n",
    "\n",
    "Fino ad ora abbiamo visto come creare un tensore, e come selezionarne alcune parti. Le operazioni che vedremo adesso rientrano nella \"composizione\" di tensori. \n",
    "- La funzione di concatenazione prende n tensori e li concatena rispetto alla dimensione (`axis`) che vogliamo. Le dimensioni dei tensori di origine devono essere tutte uguali, __eccetto la dimensione scelta__ (ma non è obbligatorio).\n",
    "- La funzione di stacking prende n tensori e li mette uno sopra l'altro rispetto a una nuova dimensione (`axis`) che vogliamo. Le domensioni dei tensori di origine devono essere __tutte__ uguali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((2, 3, 4))\n",
    "tensor_2 = tf.random.uniform((2, 6, 4))\n",
    "concat_tensor = tf.concat([tensor_1, tensor_2], axis=1)\n",
    "print(f\"Forma del nuovo tensore = {concat_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_tensor = tf.stack([tensor_1, tensor_2], axis=1)\n",
    "print(f\"Forma del nuovo tensore = {stacked_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "1. Create un tensore con numeri casuali di dimensione (2, 5, 3)\n",
    "2. Prendete l'ultimo elemento nella dimensione 1\n",
    "3. Mettetelo all'inizio alla dimensione 1 del tensore che avete affettato.\n",
    "\n",
    "_Hint_: il tensore risultante dovrà avere forma (2, 5, 3), visto che l'ultimo elemento lo stiamo _spostando_ all'inizio della dimensione. Per debuggare, delle print della proprietà `.shape` del tensore possono sempre fare comodo.\n",
    "\n",
    "_Nota_: il tensore che create deve essere `init_tensor`, mentre il tensore finale deve essere `final_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = None\n",
    "final_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_tensor.shape == (2, 5, 3):\n",
    "    if tf.reduce_all(final_tensor[:, 0] == init_tensor[:, -1]):\n",
    "        print(\"Ottimo lavoro!\")\n",
    "    else:\n",
    "        print(\"Mh, dimensione corretta ma i valori non corrispondono\")\n",
    "else:\n",
    "    print(f\"Errato, le dimensioni sono {init_tensor.shape} e {final_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operazioni coi tensori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le operazioni coi tensori sono tutte le solite operazioni matematiche comuni, ovvero somma, sottrazione, moltiplicazione e divisione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.random.normal((3, 3, 3))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((3, 2))\n",
    "tensor_2 = tf.random.normal((3, 2))\n",
    "tensor_1 + tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 @ tf.transpose(tensor_2, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((10, 3, 2))\n",
    "tensor_2 = tf.random.normal((10, 3, 2))\n",
    "tensor_1 @ tf.transpose(tensor_2, [0, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, tutto molto bello ma... è tutto qui? Giochiamo solo a fare origami di tensori?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning con TensorFlow\n",
    "\n",
    "In TensorFlow, ogni operazione che utilizziamo, dallo slicing a una qualsiasi funzione di calcolo, __crea un nuovo nodo del grafo computazionale__.\n",
    "\n",
    "![prova](https://www.easy-tensorflow.com/files/1_2.png)\n",
    "\n",
    "Il grafo computazionale rappresenta forse lo strumento più potente di TensorFlow, perché è quello che ci consente di fare learning in maniera infinitamente efficiente senza doverci preoccupare del calcolo del gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit classification con un modello lineare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo cosa possiamo fare con ciò che sappiamo fino a qui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.datasets as kds\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = kds.mnist.load_data(path='ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label is {train_y[0]}\")\n",
    "Image.fromarray(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo 60000 immagini per il training, ogni immagine è una matrice 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ma a noi interessa processare vettori, quindi __appiattiamo__ le ultime due dimensioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tf.reshape(train_x, [train_x.shape[0], -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo che valori ci sono dentro ogni immagine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_min(train_x), tf.reduce_max(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I modelli lineari (e, vedremo, anche le reti neurali) hanno difficoltà a processare dati con range di valori così ampi. Di conseguenza __riscaliamo__ tutto in modo che i valori della matrice siano fra 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora creiamo le nostre variabili per il modello di classificazione. Le variabili sono dei __tensori dotati di stato__, che ci permettono di definire quali sono i nodi all'interno del grafo computazionale su cui il gradiente va accumulato e trattenuto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = tf.Variable(tf.random.normal((784, 10))), tf.Variable(tf.zeros(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo come possiamo fare una predizione con questa roba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = train_x @ W + b\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa roba, vista in questa maniera, non significa niente. Dobbiamo tradurre questi valori in classi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(train_x @ W + b, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, adesso vediamo come ci stiamo comportando nella classificazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Il valore di accuratezza è {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbiamo un accuratezza che è intorno al 10%, che è perfettamente normale non avendo allenato il nostro modello. Questo significa che su 10 classi (le dieci cifre), ne prendiamo una a caso e, ovviamente, azzecchiamo solo una volta su 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo ad allenare il modello e vediamo che succede. Per allenare un modello ci servono:\n",
    "\n",
    "1. Una __funzione di loss__, che ci permette di misurare quanto bene/male stiamo performando sul nostro task di classificazione;\n",
    "2. Un __ottimizzatore__ che, presi i gradienti, decide come aggiornare i pesi per minimizzare il valore della loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora possiamo scrivere il nostro primo ciclo di training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500 # Questo valore specifica quante volte vogliamo iterare sul dataset per allenare il modello\n",
    "\n",
    "for e in range(epochs):\n",
    "    # Apriamo un GradientTape per memorizzare le operazioni eseguite\n",
    "    # durante il forward. Questo ci consentirà di fare la differenziazione\n",
    "    # automatica sul grafo computazionale.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Facciamo la nostra predizione\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "\n",
    "        # Calcoliamo il valore di loss\n",
    "        loss_value = loss_fn(train_y, prediction)\n",
    "    \n",
    "    # Calcoliamo i gradienti dal tape specificando rispetto a cosa\n",
    "    # vogliamo calcolarli (la loss) e su quali parametri (W e b)\n",
    "    grads = tape.gradient(loss_value, [W, b])\n",
    "\n",
    "    # Applichiamo i gradienti facendo un passo di ottimizzazione\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "    # Ora vediamo come ci stiamo comportando nella qualità della classificazione\n",
    "    prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "    if e % 20 == 0:\n",
    "        print(f\"Epoca {e}: accuratezza = {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cambiamo il metodo di ottimizzazione, possiamo fare anche molto meglio. Rifacciamo tutto da capo mettendo tutto insieme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = tf.Variable(tf.random.normal((784, 10))), tf.Variable(tf.zeros(10))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for e in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        loss_value = loss_fn(train_y, prediction)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, [W, b])\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        print(f\"Epoca {e}: accuratezza = {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provate ad allenare voi un modello lineare sul dataset che vedete di seguito. Potete approcciarlo esattamente allo stesso modo in cui abbiamo lavorato finora. L'importante è che prendiate dimestichezza con questi strumenti. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), _ = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dati di input (`train_x`) non hanno un valore minimo e massimo come nel caso delle immagini di prima. Quindi questa volta dovrete utilizzare lo StandardScaler di scikit-learn per preprocessarli e portarli in un range \"gestibile\" da parte del modello lineare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come vedete, questa volta non abbiamo numeri interi come \"target\". Questo significa che non vogliamo fare un task di classificazione ma di __regressione__. Per questa tipologia di task, la loss da usare è `\"mean_squared_error\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf_mela')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3917c018cfd92834e25e8ac39593c8ba50cba5d103d91e5ca7d5a867c45ae1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
